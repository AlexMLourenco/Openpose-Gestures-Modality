{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf  # Version 1.0.0 (some previous versions are used in past commits)\n",
    "from sklearn import metrics\n",
    "import random, time, os, json, glob, re, time\n",
    "from random import randint\n",
    "import pandas as pd\n",
    "from os.path import join\n",
    "import datetime\n",
    "\n",
    "# matplotlib inline\n",
    "from usefull_function import get_all_labels, get_class, get_id_label_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some general information we need to handle the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "{'147897': 'Drumming Fingers', '147898': 'Sliding Two Fingers Right', '147901': 'Sliding Two Fingers Down', '147916': 'Pulling Two Fingers In', '131717': 'Sliding Two Fingers Up'}\n"
    }
   ],
   "source": [
    "# How many frames there are in a video sequence, we feed to the RNN\n",
    "n_steps = 37\n",
    "\n",
    "# How many different classes there are.\n",
    "n_classes = 27\n",
    "\n",
    "# Which dataset are we looking at, choices body | hands | all\n",
    "db_type = 'all'\n",
    "\n",
    "# Where are the csv stored.\n",
    "train_labels = '../labels/train.csv'\n",
    "val_labels = '../labels/validation.csv'\n",
    "all_labels = '../labels/labels.csv'\n",
    "\n",
    "\n",
    "labels = get_id_label_dict(train_labels, val_labels)\n",
    "label_class_dict = get_all_labels(all_labels)\n",
    "\n",
    "#file_path = join('../../data/train/',db_type,'*.npy')\n",
    "file_path = join('../videos/',db_type,'*.npy')\n",
    "\n",
    "files = glob.glob(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the data, shuffling and splitting\n",
    "\n",
    "Note we can save the data in a numpy array since our dataset is not too big. \n",
    "Otherwise we need to work with an iterator that gets the array iteratively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_data = np.array([get_class(file, labels, label_class_dict) for file in files])\n",
    "\n",
    "y_original = y_data\n",
    "# Make a one hot encoding of the labels\n",
    "y_tmp = np.zeros((len(y_data), n_classes), dtype = np.int32)\n",
    "for i in range(len(y_data)):\n",
    "  y_tmp[i,y_data[i]] = 1\n",
    "\n",
    "y_data = y_tmp\n",
    "  \n",
    "X_data = np.r_[[np.load(files[i]) for i in range(len(files))]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([], dtype=int32)"
     },
     "metadata": {},
     "execution_count": 56
    }
   ],
   "source": [
    "train_size = 14000\n",
    "\n",
    "\n",
    "np.random.seed(42)\n",
    "permute = np.random.permutation(y_data.shape[0])\n",
    "y_data = y_data[permute]\n",
    "X_data = X_data[permute]\n",
    "\n",
    "y_train, y_test = y_data[:train_size], y_data[train_size:]\n",
    "X_train, X_test = X_data[:train_size], X_data[train_size:]\n",
    "y_test_true = y_original[permute][train_size:] \n",
    "permute[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([], dtype=int32)"
     },
     "metadata": {},
     "execution_count": 57
    }
   ],
   "source": [
    "permute[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph \n",
    "\n",
    "## Some general specifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "IndexError",
     "evalue": "index 0 is out of bounds for axis 0 with size 0",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-66-49e575bd139a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mtraining_data_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# 4519 training series (with 50% overlap between each serie)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mtest_data_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# 1197 test series\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mn_input\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# num input parameters per timestep\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mn_hidden\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m90\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m# Hidden layer num of features\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: index 0 is out of bounds for axis 0 with size 0"
     ]
    }
   ],
   "source": [
    "# Input Data \n",
    "#tf.reset_default_graph()  # commeted\n",
    "\n",
    "# Some graph specific numbers\n",
    "training_data_count = len(X_train)  # 4519 training series (with 50% overlap between each serie)\n",
    "test_data_count = len(X_test)  # 1197 test series\n",
    "n_input = len(X_train[0][0])  # num input parameters per timestep\n",
    "n_hidden = [90,50] # Hidden layer num of features\n",
    "\n",
    "\n",
    "\n",
    "# calculated as: decayed_learning_rate = learning_rate * decay_rate ^ (global_step / decay_steps)\n",
    "decaying_learning_rate = True\n",
    "learning_rate = 0.0025 #used if decaying_learning_rate set to False\n",
    "init_learning_rate = 0.002\n",
    "decay_rate = 0.96 #the base of the exponential in the decay\n",
    "decay_steps = 100000 #used in decay every 60000 steps with a base of 0.96\n",
    "\n",
    "\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "lambda_loss_amount = 0.0015\n",
    "\n",
    "epochs = 50\n",
    "training_iters = training_data_count *epochs  # Loop number of epoch through the data set.\n",
    "batch_size = 512\n",
    "display_iter = batch_size*64 # To show test set accuracy during training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_batch_size(_train, _labels, _unsampled, batch_size):\n",
    "    # Return a shuffled batch\n",
    "    \n",
    "    index = np.random.choice(_unsampled,size = batch_size)\n",
    "    batch_s = _train[index]\n",
    "    batch_labels = _labels[index]\n",
    "    _unsampled = np.delete(_unsampled, index)\n",
    "    \n",
    "    return batch_s, batch_labels, _unsampled\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## The graph definition\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'placeholder'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-60-262dabf5ed70>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Graph input/output\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_steps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_input\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_classes\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#y_hot = tf.one_hot(tf.cast(y,tf.int32),n_classes)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'placeholder'"
     ]
    }
   ],
   "source": [
    "# Graph input/output\n",
    "x = tf.placeholder(tf.float32, [None, n_steps, n_input])\n",
    "y = tf.placeholder(tf.int32, [None, n_classes])\n",
    "#y_hot = tf.one_hot(tf.cast(y,tf.int32),n_classes)\n",
    "\n",
    "\n",
    "with tf.name_scope('Input') as _:\n",
    "  xt = tf.transpose(x, [1, 0, 2])  # permute n_steps and batch_size\n",
    "  xr = tf.reshape(xt, [-1, n_input])  \n",
    "  # Rectifies Linear Unit activation function used\n",
    "    \n",
    "with tf.name_scope('First_fc_layer') as _:\n",
    "  y_1 = tf.layers.dense(xr, n_hidden[0], \n",
    "  bias_initializer=tf.truncated_normal_initializer(stddev=0.1), \n",
    "                  kernel_initializer=tf.truncated_normal_initializer(stddev=0.1), \n",
    "                  activation=tf.nn.relu, name='layer_1')\n",
    "    \n",
    "  # So the reader might be wondering why we take the transpose before the reshape\n",
    "  # This has to do with how the lstm wants to eat the data.\n",
    "  # the lstm want n_frames tensors, where each tensor is of size batchsize x num_features\n",
    "  # So basically first all the 1st frames then all the 2nd frames etc.\n",
    "  _X_split = tf.split(y_1, n_steps, 0) \n",
    "\n",
    "    \n",
    "with tf.name_scope('LSTM') as _:\n",
    "  # Define two stacked LSTM cells (two recurrent layers deep) with tensorflow\n",
    "  lstm_cell_1 = tf.contrib.rnn.BasicLSTMCell(n_hidden[0], forget_bias=1.0, state_is_tuple=True)\n",
    "  lstm_cell_2 = tf.contrib.rnn.BasicLSTMCell(n_hidden[0], forget_bias=1.0, state_is_tuple=True)\n",
    "  lstm_cells = tf.contrib.rnn.MultiRNNCell([lstm_cell_1, lstm_cell_2], state_is_tuple=True)\n",
    "  outputs, states = tf.contrib.rnn.static_rnn(lstm_cells, _X_split, dtype=tf.float32)\n",
    "    \n",
    "  # A single output is produced, in style of \"many to one\" classifier, refer to http://karpathy.github.io/2015/05/21/rnn-effectiveness/ for details\n",
    "  lstm_last_output = outputs[-1]\n",
    "    \n",
    "    \n",
    "with tf.name_scope('Second_fc_layer') as _:\n",
    "  y_2 = tf.layers.dense(lstm_last_output, n_hidden[1], \n",
    "                          bias_initializer=tf.truncated_normal_initializer(stddev=0.1), \n",
    "                          kernel_initializer=tf.truncated_normal_initializer(stddev=0.1), \n",
    "                          activation=tf.nn.relu, name='layer_2')\n",
    "    \n",
    "    \n",
    "with tf.name_scope('Last_fc_layer') as _:\n",
    "  out = tf.layers.dense(y_2, n_classes, \n",
    "                                  bias_initializer=tf.truncated_normal_initializer(stddev=0.1), \n",
    "                                  kernel_initializer=tf.truncated_normal_initializer(stddev=0.1), \n",
    "                                  activation=None, \n",
    "                                  name='final_layer')\n",
    "\n",
    "\n",
    "with tf.name_scope('Output') as _:\n",
    "  pred = out\n",
    "  \n",
    "with tf.name_scope('Optimizer') as _:\n",
    "  # Loss, optimizer and evaluation\n",
    "  # The regularization term\n",
    "  l2 = lambda_loss_amount * sum(tf.nn.l2_loss(tf_var) for tf_var in tf.trainable_variables()) \n",
    "  \n",
    "  cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "                          labels=y, \n",
    "                          logits=pred)) + l2 # Softmax loss\n",
    "  if decaying_learning_rate:\n",
    "    learning_rate = tf.train.exponential_decay(init_learning_rate, global_step*batch_size, decay_steps, decay_rate, staircase=True)\n",
    "\n",
    "\n",
    "  #decayed_learning_rate = learning_rate * decay_rate ^ (global_step / decay_steps) #exponentially decayed learning rate\n",
    "  optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost,global_step=global_step) # Adam Optimizer\n",
    "  tf.summary.scalar('cost', cost)\n",
    "  \n",
    "  \n",
    "with tf.name_scope('Accuracy') as _:\n",
    "  correct_pred = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))\n",
    "  accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "  tf.summary.scalar('accuracy', accuracy)\n",
    "  \n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---\n",
    "\n",
    "# Training of the Graph\n",
    "\n",
    "## Only run this part if you want to train your own model\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "module 'tensorflow_core.summary' has no attribute 'merge_all'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-61-7b30dd2a155e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0msummary_dir\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'./logs/'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mdb_type\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mtime_now\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mmerged\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmerge_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mtrain_writer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFileWriter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msummary_dir\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'/train'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow_core.summary' has no attribute 'merge_all'"
     ]
    }
   ],
   "source": [
    "# Inititialize writers\n",
    "time_now = datetime.datetime.strftime(datetime.datetime.now(), '%H%M%S')\n",
    "summary_dir = './logs/'+db_type+time_now\n",
    "\n",
    "merged = tf.summary.merge_all()\n",
    "\n",
    "train_writer = tf.summary.FileWriter(summary_dir + '/train')\n",
    "test_writer=  tf.summary.FileWriter(summary_dir + '/test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'global_variables_initializer'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-62-5690ab7342f3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#sess = tf.InteractiveSession(config=tf.ConfigProto(log_device_placement=True))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0minit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# Perform Training steps with \"batch_size\" amount of data at each loop.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# Elements of each batch are chosen randomly, without replacement, from X_train,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'global_variables_initializer'"
     ]
    }
   ],
   "source": [
    "#sess = tf.InteractiveSession(config=tf.ConfigProto(log_device_placement=True))\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Perform Training steps with \"batch_size\" amount of data at each loop. \n",
    "# Elements of each batch are chosen randomly, without replacement, from X_train, \n",
    "# restarting when remaining datapoints < batch_size\n",
    "step = 1\n",
    "best_test_acc = 0\n",
    "time_start = time.time()\n",
    "unsampled_indices = list(range(0,len(X_train)))\n",
    "start = time.time()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "  \n",
    "  sess.run(init)\n",
    "  sess.graph.finalize()\n",
    "  writer = tf.summary.FileWriter(summary_dir, sess.graph)\n",
    "  while step * batch_size <= training_iters:\n",
    "    if len(unsampled_indices) < batch_size:\n",
    "      unsampled_indices = list(range(0,len(X_train))) \n",
    "    batch_xs, batch_ys, unsampled_indicies = extract_batch_size(X_train, y_train, unsampled_indices, batch_size)\n",
    "    \n",
    "    # Fit training using batch data\n",
    "    _, loss_trn, acc_trn = sess.run([optimizer, cost, accuracy], feed_dict={x: batch_xs, y: batch_ys})\n",
    "    \n",
    "    summary = sess.run(merged, feed_dict={x: batch_xs, y: batch_ys}) \n",
    "    train_writer.add_summary(summary, step)\n",
    "    \n",
    "    \n",
    "    # Evaluate network only at some steps for faster training: \n",
    "    if (step*batch_size % display_iter == 0) or (step == 1) or (step * batch_size > training_iters):\n",
    "        \n",
    "        # Evaluation on the test set (no learning made here - just evaluation for diagnosis)\n",
    "        loss_tst, acc_tst = sess.run([cost, accuracy], feed_dict={x: X_test, y: y_test})\n",
    "        summary = sess.run(merged, feed_dict={x: X_test, y: y_test})\n",
    "        \n",
    "        test_writer.add_summary(summary, step)\n",
    "        \n",
    "        \n",
    "        print('\\r Iter: {},  Accuracy test: {}, Accuracy train: {}, time: {}'.format(step*batch_size, acc_tst, acc_trn, time.time()-start))\n",
    "        \n",
    "        if(acc_tst > best_test_acc):\n",
    "          print(\"\\tbetter network stored,\", acc_tst, \">\", best_test_acc)\n",
    "          best_test_acc = acc_tst\n",
    "          saver.save(sess=sess, save_path=summary_dir + '/bestNetwork')\n",
    "          \n",
    "        \n",
    "        start = time.time()\n",
    "    step += 1\n",
    "  print(\"Optimization Finished!\")\n",
    "\n",
    "  # Accuracy for test data\n",
    "  \n",
    "  saver.restore(sess=sess, save_path=summary_dir + '/bestNetwork')\n",
    "  print(\"best training accuracy:\", sess.run(accuracy, feed_dict={x: X_train, y: y_train}),\n",
    "  \"best test accuracy: \", sess.run(accuracy, feed_dict={x: X_test, y: y_test}))\n",
    "\n",
    "time_stop = time.time()\n",
    "print((\"TOTAL TIME:  {}\".format(time_stop - time_start)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Get a previous best network\n",
    "\n",
    "For this to work we need the definition of the graph above.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'Session'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-63-b573f5eacba5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m   \u001b[0msaver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msave_path\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'logs/all180232'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'/bestNetwork'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m   \u001b[0mprediction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'Session'"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "  saver.restore(sess=sess, save_path='logs/all180232' + '/bestNetwork')\n",
    "  prediction = sess.run(pred, feed_dict={x: X_test, y: y_test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'prediction' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-64-bf7e9e2e297f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprediction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprediction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprediction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'prediction' is not defined"
     ]
    }
   ],
   "source": [
    "prediction = prediction.argmax(axis = 1)\n",
    "prediction.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "weighted\n"
    },
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'prediction' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-65-9a377b432339>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mavg\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'weighted'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'macro'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'micro'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m   \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mavg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m   \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Accuracy: {:.2f}%\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprediction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m   \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Precision: {:.2f}%\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprecision_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprediction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mavg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m   \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Recall: {:.2f}%\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecall_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprediction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mavg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'prediction' is not defined"
     ]
    }
   ],
   "source": [
    "for avg in ['weighted', 'macro', 'micro']:\n",
    "  print(avg)\n",
    "  print(\"Accuracy: {:.2f}%\".format(100*metrics.accuracy_score(y_test_true, prediction)))\n",
    "  print(\"Precision: {:.2f}%\".format(100*metrics.precision_score(y_test_true, prediction, average=avg)))\n",
    "  print(\"Recall: {:.2f}%\".format(100*metrics.recall_score(y_test_true, prediction, average=avg)))\n",
    "  print(\"f1_score: {:.2f}%\".format(100*metrics.f1_score(y_test_true, prediction, average=avg)))\n",
    "\n",
    "confusion_matrix = metrics.confusion_matrix(y_test_true, prediction)\n",
    "\n",
    "label_axis = sorted(label_class_dict, key = label_class_dict.__getitem__)\n",
    "\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.figure(figsize=(15, 15))\n",
    "im = plt.imshow(confusion_matrix, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "\n",
    "plt.title(\"Confusion matrix \"+db_type, fontsize = 22)\n",
    "cb = plt.colorbar(im,fraction=0.046, pad=0.04)\n",
    "cb.set_label('Number of predictions', fontsize = 20)\n",
    "tick_marks = np.arange(n_classes)\n",
    "plt.xticks(tick_marks, label_axis, rotation=-90)\n",
    "plt.yticks(tick_marks, label_axis)\n",
    "plt.ylabel('True label', fontsize = 20)\n",
    "plt.xlabel('Predicted label', fontsize = 20)\n",
    "plt.savefig('confusion_matrix_'+db_type, bbox_inches='tight' )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}